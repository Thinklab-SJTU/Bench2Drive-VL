"use strict";(self.webpackChunkb_2_dvl_docs_site=self.webpackChunkb_2_dvl_docs_site||[]).push([[6805],{1927:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"tutorial/closed-loop-inference","title":"Closed-Loop Inference","description":"When doing closed-loop inference, Bench2Drive-VL will apply DriveCommenter to generate real time VQAs, then let the VLM to control the ego vehicle. Question details, ground truths and VLM\'s answers of VQAs will be saved under ./output for latter evaluation. For planning section, we utilize the original Bench2Drive metrics.","source":"@site/versioned_docs/version-0.0/tutorial/closed-loop-inference.md","sourceDirName":"tutorial","slug":"/tutorial/closed-loop-inference","permalink":"/Bench2Drive-VL/docs/tutorial/closed-loop-inference","draft":false,"unlisted":false,"tags":[],"version":"0.0","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Generate VQAs","permalink":"/Bench2Drive-VL/docs/tutorial/generate-vqa"},"next":{"title":"Open-Loop Inference","permalink":"/Bench2Drive-VL/docs/tutorial/open-loop-inference"}}');var i=t(4848),o=t(8453);const s={sidebar_position:3},a="Closed-Loop Inference",l={},c=[{value:"Write a VLM config file",id:"write-a-vlm-config-file",level:2},{value:"Write a start up script",id:"write-a-start-up-script",level:2},{value:"Start VLM Server",id:"start-vlm-server",level:2},{value:"Start to inference",id:"start-to-inference",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"closed-loop-inference",children:"Closed-Loop Inference"})}),"\n",(0,i.jsxs)(n.p,{children:["When doing closed-loop inference, Bench2Drive-VL will apply DriveCommenter to generate real time VQAs, then let the VLM to control the ego vehicle. Question details, ground truths and VLM's answers of VQAs will be saved under ",(0,i.jsx)(n.code,{children:"./output"})," for latter evaluation. For planning section, we utilize the original Bench2Drive metrics."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Overall Structure",src:t(9315).A+"",width:"3811",height:"2702"})}),"\n",(0,i.jsx)(n.h2,{id:"write-a-vlm-config-file",children:"Write a VLM config file"}),"\n",(0,i.jsx)(n.p,{children:"Write a vlm config file:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",metastring:'title="vlm_config.json"',children:'{\n    "TASK_CONFIGS": {\n        "FRAME_PER_SEC": 10 // sensor saving frequency\n    },\n    "INFERENCE_BASICS": {\n        "INPUT_WINDOW": 1, // frame count of given image input\n        "CONVERSATION_WINDOW": 1, // not used anymore, to be removed\n        "USE_ALL_CAMERAS": false, // true if use all cameras as input\n        "USE_BEV": false, // true if use bev as input\n        "NO_HISTORY_MODE": false // do not inherit context of previous VQAs\n    },\n    "CHAIN": { // for inference\n        "NODE": [19, 15, 7, 24, 13, 47, 8, 43, 50],\n        "EDGE": { // "pred": succ\n            "19": [24, 13, 8],\n            "15": [7, 8],\n            "7": [8],\n            "24": [13, 47],\n            "13": [47, 8, 43],\n            "47": [8],\n            "8": [43],\n            "43": [50],\n            "50": []\n        },\n        "INHERIT": { // inherit context from last frame\n            "19": [43, 7],\n            "15": [7]\n        },\n        "USE_GT": [24] // questions which use ground truth as answer\n    },\n    "CONTROL_RATE": 2.0, // intervene frequency of vlm\n    "MODEL_NAME": "api", // model name, please check out supported models\n    "MODEL_PATH": "../model_zoo/your_model", // model path\n    "GPU_ID": 0, // the gpu model runs on\n    "PORT": 7023, // web port\n    "IN_CARLA": true,\n    "USE_BASE64": true, // if false, local path is used for transmitting images\n    "NO_PERC_INFO": false // do not pass extra perception info to vlm via prompt\n}\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Question id",type:"info",children:(0,i.jsxs)(n.p,{children:["Please refer to ",(0,i.jsx)(n.a,{href:"/Bench2Drive-VL/docs/references/supported-vqas",children:"supported vqas"})," for question ids."]})}),"\n",(0,i.jsx)(n.admonition,{title:"Supported VLMs",type:"info",children:(0,i.jsxs)(n.p,{children:["Please refer to ",(0,i.jsx)(n.a,{href:"/Bench2Drive-VL/docs/references/adapt-vlm#supported-list",children:"supported models"}),"."]})}),"\n",(0,i.jsx)(n.admonition,{title:"Required",type:"danger",children:(0,i.jsxs)(n.p,{children:["Make sure you include ",(0,i.jsx)(n.strong,{children:"question 50"})," because the action module requires its answer."]})}),"\n",(0,i.jsx)(n.h2,{id:"write-a-start-up-script",children:"Write a start up script"}),"\n",(0,i.jsx)(n.p,{children:"Write a start up script for inferencing framework:"}),"\n",(0,i.jsx)(n.admonition,{title:"Quickstart",type:"tip",children:(0,i.jsxs)(n.p,{children:["If you want a quickstart, you can set ",(0,i.jsx)(n.code,{children:"MINIMAL=1"})," to run Bench2Drive-VL without VLM. In this mode, DriveCommenter will take control of the ego vehicle."]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",metastring:'title="startup.sh"',children:'#!/bin/bash\nBASE_PORT=20082 # CARLA port\nBASE_TM_PORT=50000 # CARLA traffic manager port\nBASE_ROUTES=./leaderboard/data/bench2drive220 # path to your route xml\nTEAM_AGENT=leaderboard/team_code/data_agent.py # path to your agent, in B2DVL, the agent is fixed, so don\'t modify this\nBASE_CHECKPOINT_ENDPOINT=./my_checkpoint # path to the checkpoint file with saves sceanario running process and results. \n# If not exist, it will be automatically created.\nSAVE_PATH=./eval_v1/ # the directory where seonsor data is saved.\nGPU_RANK=0 # the gpu carla runs on\nVLM_CONFIG=/path/to/your_vlm_config.json \nPORT=$BASE_PORT\nTM_PORT=$BASE_TM_PORT\nROUTES="${BASE_ROUTES}.xml"\nCHECKPOINT_ENDPOINT="${BASE_CHECKPOINT_ENDPOINT}.json"\nexport MINIMAL=0 # if MINIMAL > 0, DriveCommenter takes control of the ego vehicle,\n# and vlm server is not needed\nexport EARLY_STOP=80 # When getting baseline data, we used a 80s early-stop to avoid wasting time on failed scenarios. You can delete this line to disable early-stop. \nbash leaderboard/scripts/run_evaluation.sh $PORT $TM_PORT 1 $ROUTES $TEAM_AGENT "." $CHECKPOINT_ENDPOINT $SAVE_PATH "null" $GPU_RANK $VLM_CONFIG\n'})}),"\n",(0,i.jsx)(n.admonition,{title:"Data Savings",type:"info",children:(0,i.jsxs)(n.p,{children:["When running closed-loop inference, sensor data will be saved under ",(0,i.jsx)(n.code,{children:"${SAVE_PATH}/model_name+input/"}),", VQA generated by DriveCommenter will be saved under ",(0,i.jsx)(n.code,{children:"outputs/vqagen/model_name+input/"}),", VLM's inference results will be saved under ",(0,i.jsx)(n.code,{children:"outputs/infer_results/model_name+input/"}),"."]})}),"\n",(0,i.jsx)(n.h2,{id:"start-vlm-server",children:"Start VLM Server"}),"\n",(0,i.jsx)(n.admonition,{title:"Quickstart",type:"tip",children:(0,i.jsxs)(n.p,{children:["You don't need to do this step if you set ",(0,i.jsx)(n.code,{children:"MINIMAL=1"}),"."]})}),"\n",(0,i.jsx)(n.p,{children:"Start the web server for your vlm:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"python ./B2DVL_Adapter/web_interact_app.py --config /path/to/your/vlm_config.json\n"})}),"\n",(0,i.jsx)(n.h2,{id:"start-to-inference",children:"Start to inference"}),"\n",(0,i.jsx)(n.p,{children:"Run the start up script you just wrote:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-shell",children:"bash ./startup.sh\n"})})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const i={},o=r.createContext(i);function s(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:n},e.children)}},9315:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/framework-2dc0e0fb1aeb016cc956e4c4c06e78cc.png"}}]);