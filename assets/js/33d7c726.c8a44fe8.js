"use strict";(self.webpackChunkb_2_dvl_docs_site=self.webpackChunkb_2_dvl_docs_site||[]).push([[9768],{8199:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>d,default:()=>m,frontMatter:()=>a,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"references/adapt-vlm","title":"Adapt New VLMs","description":"You can add your new vlms to B2DVL. Before doing that, please check out our supported list first.","source":"@site/versioned_docs/version-0.0/references/adapt-vlm.md","sourceDirName":"references","slug":"/references/adapt-vlm","permalink":"/Bench2Drive-VL/docs/references/adapt-vlm","draft":false,"unlisted":false,"tags":[],"version":"0.0","sidebarPosition":5,"frontMatter":{"sidebar_position":5},"sidebar":"tutorialSidebar","previous":{"title":"Evaluation Configurations","permalink":"/Bench2Drive-VL/docs/references/eval-config"},"next":{"title":"Evaluation Details","permalink":"/Bench2Drive-VL/docs/references/eval-details"}}');var s=t(4848),i=t(8453);const a={sidebar_position:5},d="Adapt New VLMs",l={},o=[{value:"Supported list",id:"supported-list",level:2},{value:"Add a new VLM",id:"add-a-new-vlm",level:2},{value:"Implement VLM interface",id:"implement-vlm-interface",level:3},{value:"Register new VLM",id:"register-new-vlm",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"adapt-new-vlms",children:"Adapt New VLMs"})}),"\n",(0,s.jsx)(n.p,{children:"You can add your new vlms to B2DVL. Before doing that, please check out our supported list first."}),"\n",(0,s.jsx)(n.h2,{id:"supported-list",children:"Supported list"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"model"}),(0,s.jsx)(n.th,{children:"key"}),(0,s.jsx)(n.th,{children:"single frame, front-cam"}),(0,s.jsx)(n.th,{children:"single frame, multi-cam"}),(0,s.jsx)(n.th,{children:"single frame, BEV"}),(0,s.jsx)(n.th,{children:"multi frame, front-cam"}),(0,s.jsx)(n.th,{children:"multi frame, multi-cam"}),(0,s.jsx)(n.th,{children:"multi frame, BEV"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Qwen2.5VL"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Qwen2.5VL"})}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"LLaVA_NeXT"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"LLaVANeXT"})}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Gemma3"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Gemma"})}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"InternVL3"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"InternVL"})}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u274c"}),(0,s.jsx)(n.td,{children:"\u274c"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Janus-pro"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"Janus-Pro"})}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"API (openai template)"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"api"})}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"}),(0,s.jsx)(n.td,{children:"\u2705"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["You can use their model keys to use these models. Furthermore, you can use ",(0,s.jsx)(n.code,{children:"gt"})," as model name to use grounf truth directly, but it will be more convenient to use ",(0,s.jsx)(n.code,{children:"MINIMAL=1"})," since you don't need to set up VLM server if using this option."]}),"\n",(0,s.jsx)(n.h2,{id:"add-a-new-vlm",children:"Add a new VLM"}),"\n",(0,s.jsx)(n.h3,{id:"implement-vlm-interface",children:"Implement VLM interface"}),"\n",(0,s.jsxs)(n.p,{children:["To add a new VLM, you may create a python file under ",(0,s.jsx)(n.code,{children:"B2DVL_Adapter/models"})," to implement corresponding interface."]}),"\n",(0,s.jsx)(n.p,{children:"Take Qwen2.5VL's interface as an example:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="qwen25.py"',children:'from .VLMInterface import VLMInterface\nfrom .interact_utils import get_image_descriptions, get_carla_image_descriptions\nfrom PIL import Image\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\nimport torch\n\ndef image_template(image_path, use_base64):\n    return {\n        "type": "image",\n        "image": image_path if use_base64 else f"file://{image_path}"\n    }\n\nclass Qwen25Interface(VLMInterface):\n    def initialize(self, gpu_id: int, use_all_cameras: bool, no_history: bool, \n                    input_window: int, frame_rate: int, model_path: str, use_bev: bool=False, \n                    in_carla: bool=False, use_base64: bool=False):\n        print(f"Initializing Qwen2.5VL on GPU {gpu_id}...")\n        # Load model, weights, and allocate resources here\n        self.in_carla = in_carla\n        self.use_bev = use_bev\n        self.use_all_cameras = use_all_cameras\n        self.input_window = input_window\n        self.no_history = no_history\n        self.gpu_id = gpu_id\n        self.model_path = model_path\n        self.frame_rate = frame_rate\n        self.use_base64 = use_base64\n\n        torch.cuda.set_device(self.gpu_id)\n        self.device = torch.device(f"cuda:{self.gpu_id}")\n\n        self.multi_image_flag = (self.use_all_cameras or (self.no_history == False and self.input_window > 1))\n        \n        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n            self.model_path, low_cpu_mem_usage=True\n        )\n        self.model.to(f"cuda:{gpu_id}")\n        self.processor = AutoProcessor.from_pretrained(self.model_path)\n\n        print(f"Qwen2.5VL loaded on GPU {gpu_id} successfully")\n    \n    \n    def get_image_descriptions(self, images_dict, image_frame_list, start_frame, end_frame):\n        """\n        Returns content list about descriptions of all images in range (start_frame, end_frame]\n        """\n        if self.in_carla:\n            return get_carla_image_descriptions(images_dict=images_dict,\n                                                image_frame_list=image_frame_list,\n                                                start_frame=start_frame,\n                                                end_frame=end_frame,\n                                                frame_rate=self.frame_rate,\n                                                template_func=image_template,\n                                                use_all_cameras=self.use_all_cameras,\n                                                use_bev=self.use_bev,\n                                                use_base64=self.use_base64)\n        else:\n            return get_image_descriptions(images_dict=images_dict,\n                                        image_frame_list=image_frame_list,\n                                        start_frame=start_frame,\n                                        end_frame=end_frame,\n                                        frame_rate=self.frame_rate,\n                                        template_func=image_template,\n                                        use_all_cameras=self.use_all_cameras,\n                                        use_base64=self.use_base64)\n\n    def interact(self, bubble, conversation):\n\n        torch.cuda.set_device(self.gpu_id)\n        self.device = torch.device(f"cuda:{self.gpu_id}")\n\n        input_conversation = []\n        \n        images_list = bubble.get_full_images()\n        image_frame_list = sorted(images_list.keys())\n\n        start_frame = bubble.frame_number\n        current_frame = bubble.frame_number\n        if conversation is not None and len(conversation) > 0:\n            start_frame = conversation[0].frame_number\n\n        prev_frame = -1\n\n        # context\n        context_str = ""\n        if self.no_history == False:\n            # all in one context\n            context_str = ""\n            context_bb = {\n                "role": "context",\n                "content": []\n            }\n\n            for bb in conversation:\n                is_user = (bb.actor == "User")\n                \n                if is_user and prev_frame < bb.frame_number:\n                    image_content, _ = self.get_image_descriptions(images_list, image_frame_list,\n                                                        prev_frame, bb.frame_number)\n                    prev_frame = bb.frame_number\n\n                    if context_str is not None and context_str != "":\n                        frame_content = {\n                            "type": "text",\n                            "text": context_str\n                        }\n                        context_bb[\'content\'].append(frame_content)\n                        context_str = ""\n\n                    context_bb[\'content\'].extend(image_content)\n                \n                header = "Q" if is_user else "A"\n                context_str += f"{header}(frame {bb.frame_number}): {bb.words}\\n"\n            \n            if context_str is not None and context_str != "":\n                frame_content = {\n                    "type": "text",\n                    "text": context_str\n                }\n                context_bb[\'content\'].append(frame_content)\n                context_str = ""\n            \n            input_conversation.append(context_bb)\n                \n        bb_dict = {\n            "role": "user",\n            "content": []\n        }\n        if prev_frame < current_frame:\n            image_content, _ = self.get_image_descriptions(images_list, image_frame_list,\n                                                    prev_frame, current_frame)\n            prev_frame = current_frame\n            bb_dict[\'content\'].extend(image_content)\n        bb_dict[\'content\'].append({\n            "type": "text",\n            "text": f"Q(frame {bubble.frame_number}): {bubble.get_full_words()}"\n        })\n        input_conversation.append(bb_dict)\n\n        input_image_files = []\n        for frame_number in image_frame_list:\n            if (frame_number < current_frame and self.no_history == False) or \\\n                frame_number == current_frame:\n                if self.use_all_cameras:\n                    for key in images_list[frame_number].keys():\n                        if key in [\'CAM_FRONT_CONCAT\', \'CAM_BACK_CONCAT\']:\n                            input_image_files.append(images_list[frame_number][key])\n                else:\n                    input_image_files.append(images_list[frame_number][\'CAM_FRONT\'])\n\n        prompts = self.processor.apply_chat_template(input_conversation, tokenize=False, add_generation_prompt=True)\n        image_inputs, video_inputs = process_vision_info(input_conversation)\n\n        inputs = self.processor(text=[prompts], images=image_inputs, videos=video_inputs,\n                           padding=True, return_tensors="pt").to(self.device)\n        generated_ids = self.model.generate(**inputs, max_new_tokens=1024)\n\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        output_text = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )\n\n        result = output_text\n        \n        if isinstance(result, list):\n            result = result[0]\n        \n        return result\n'})}),"\n",(0,s.jsx)(n.h3,{id:"register-new-vlm",children:"Register new VLM"}),"\n",(0,s.jsxs)(n.p,{children:["To make your new VLM usable, you need to register your VLM interface in ",(0,s.jsx)(n.code,{children:"B2DVL_Adapter/models/register.py"})]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="register.py"',children:'MODEL_MAP = {\n    "gt": "VLMInterface",\n    "LLaVANeXT": "LLaVANeXTInterface",\n    "Qwen2.5VL": "Qwen25Interface",\n    "api": "VLMAPIInterface",\n    "Gemma": "GemmaInterface",\n    "Janus-Pro": "JanusProInterface",\n    "InternVL": "InternVLInterface"\n    # Add other models as you need\n}\n\ndef get_model_interface(model_name):\n    """\n    Retrieve the appropriate model interface class based on the model name.\n    :param model_name: Name of the model.\n    :return: An instance of the corresponding model interface.\n    """\n    if model_name not in MODEL_MAP:\n        raise ValueError(f"Model {model_name} is not supported. Available models: {list(MODEL_MAP.keys())}")\n    \n    class_name = MODEL_MAP[model_name]\n    \n    # Lazy import based on model_name\n    if model_name == "gt":\n        from .VLMInterface import VLMInterface\n        return VLMInterface()\n    elif model_name == "LLaVANeXT":\n        from .LLaVA_NeXT import LLaVANeXTInterface\n        return LLaVANeXTInterface()\n    elif model_name == "Qwen2.5VL":\n        from .qwen25 import Qwen25Interface\n        return Qwen25Interface()\n    elif model_name == "api":\n        from .vlm_api import VLMAPIInterface\n        return VLMAPIInterface()\n    elif model_name == "Gemma":\n        from .gemma import GemmaInterface\n        return GemmaInterface()\n    elif model_name == "Janus-Pro":\n        from .janus import JanusProInterface\n        return JanusProInterface()\n    elif model_name == "InternVL":\n        from .intern import InternVLInterface\n        return InternVLInterface()\n    # As new interface as you need\n'})})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>d});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function d(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);